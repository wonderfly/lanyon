---
layout: post
title: UTLK - CHAPTER 2 Memory Addressing
category: [ utlk ]
---

Note: The subtitles in this chapter are reorganized for the ease of my own
understanding. I find them more logically ordered this way than they are in the
book.

# Memory Addresses
This chapter focuses on the Intel 80x86 microprocessor memory addressing
techniques, but they should generally apply to most other processors too.

At the highest level, you have a great amount of memory cells that can be used
to store instructions to be executed by the CPU and data to be read by the CPU.
You need to tell the CPU how to access the contents of each cell so you give
them "addresses", just like every house in America has an address so mails can
be delivered. Each memory address typically points to a single _byte_ in memory.

## Segmentation
Intel thinks it is a good idea to split a big program into small logical
_segments_, each to store partial information about the program. For example,
there can be a _code segment_ for compiled instructions of the program, while
all global and static variables may reside a separate segment called the _data
segment_.  Similarly, there can be _stack segment_ designated for a process'
stack at runtime. This, when applied to the address space of a running process,
splits the entire memory into multiple segments too, each consists of a chunk of
addresses.

## Segment Selector
With segmentation, to address a memory location, a pair of identifiers are used:
`(Segmentation Selector, Offset)`. Segmentation Selector is a 16-bit number that
identifies which of the many segments this address lies in, and "Offset" tells
the distance from the start of the selected segment.

## Segment Descriptor
Other than a base address, to precisely describe a segment, at least its "size"
needs to be known. There are other characteristics of a segment that the kernel
finds useful, such as which privilege level is required to access addresses in
a segment's range, is the segment for data or code, etc. These details together,
are stored in a data structure called the _Segmentation Descriptor_.

## Global Descriptor Table (GDT) and Local Descriptor Table (LDT)
Now that we have a set of segments, each with its descriptor, we need an array
to store all these descriptors. This array is called a Descriptor Table. There
is usually a Global Descriptor Table or GDT that's shared by all processes (they
use the same segments for addressing), and optionally a per-process table called
the Local Descriptor Table.

Both tables are stored in memory themselves. To locate them, their addresses are
stored in two registers: `gdtr` and `ldtr`, respectively.

## Logical Address and Linear Address
In Intel's design, all addresses used in programs should be "Logical Addresses",
which are `(Segmentation Selector, Offset)` pairs. But the CPU doesn't
understand these pairs, so there is a designated hardware component that
_translates_ logical addresses into _Linear Addresses_, which are 32-bit
unsigned integers that range from 0x00000000 to 0xffffffff, for a memory of 4
gig bytes. This hardware component is called the _Segmentation Unit_.

## Segmentation Unit
Segmentation unit's sole purpose is to translate logical addresses into linear
addresses. It takes a `(Segment Selector, Offset)` pair, from which it gets the
_index_ into the Descriptor Table (array) of the segment to select, which then
gives the starting (base) address of that segment. Then by multiplying Offset by
8 (each descriptor in the Descriptor Table is 8-byte long) and adding the
product to the base, it gets a linear address.

## Fast Address Translation
As you can see from above section, each address translation involves one read of
the Descriptor Table (to get the descriptor of the selected segment), which sits
in memory. Because the descriptor of a segment rarely changes, it's possible to
speed up translations by loading the descriptor (8 byte) into a dedicated
register, so subsequent translations don't need to read memory, which is a
magnitude slower than reading a register.

## Segmentation in Linux
Though the concept of segmentation and splitting programs into logically related
chunks is kind of enforced by Intel processors, Linux doesn't use segmentation
much. In fact, it defines barely a handful of segments even though the maximum
allowed number is 2^13 (13-bit used for _index_ in Segment Selector).

Four main segments Linux uses are: Kernel Code Segment, Kernel Data Segment,
User Code Segment, and User Data Segment. The Segment Selector values of these
are defined by the macros `__KERNEL_CS`, `__KERNEL_DS`, `__USER_CS` and
`__USER_DS`. To address the kernel code segment, for example, the kernel simply
loads the value defined by `__KERNEL_CS` into the segmentation register.

The descriptors of each segment are as follows:

| Segment     | Base      | G   | Limit   | S   | Type  | DPL | D/B | P   |
|:---         |:---       |:--- |:---     |:--- |:---   |:--- |:--- |:--- |
| user code   | 0x00000000|1    | 0xffff  | 1   | 10    | 3   | 1   | 1   |
| user data   | 0x00000000|1    | 0xffff  | 1   | 2     | 3   | 1   | 1   |
| kernel code | 0x00000000|1    | 0xffff  | 1   | 10    | 0   | 1   | 1   |
| kernel data | 0x00000000|1    | 0xffff  | 1   | 2     | 0   | 1   | 1   |

As you can see all four segments start at address 0. This is not a mistake. In
Linux, a logical address always coincides with a linear address. Again, Linux is
not a big fan of segmentation and spends more of its design in an alternative
paging mechanism - paging - which we will talk about next.

## The Linux GDT
As mentioned above, the GDT is an array of Segment Descriptor structs. In Linux,
the array is defined as `cpu_gdt_table`, while the address and size of it are
defined in the `cpu_gdt_descr` array. It is an array because there can be
multiple GDTs: on multi-processor systems each CPU has a GDT. These values are
used to initialize the `gdtr` register.

Both arrays are 32-element long, which include 18 valid segment descriptors,
and 14 null, unused or reserved entries. Unused entries are there so that
segment descriptors accessed together are kept in the 32-byte hardware cache
line.

## The Linux LDTs
These are similar to GDTs and are not commonly used by Linux applications except
for those that run Windows applications, e.g., Wine.

# Paging in Hardware
Just as the segmentation unit translates logical addresses into linear
addresses, another hardware unit, the _paging unit_, translates linear addresses
into physical addresses.

## Page, Page Frame and Page Table
For efficiency, linear addresses are grouped into fixed-length chunks called
_pages_. Contiguous linear addresses are mapped into contiguous physical
addresses. Each of these groups is called a physical _page frame_. Note that
while pages are blocks of data, page frames are physical constitute of the main
memory.

The data structure that maps pages to page frames is called _page table_.

## Regular Paging
The word "regular" is relative to the extended or more advanced paging
techniques that are required for special cases that we will cover later in this
chapter. This section is about the fundamental idea of how paging is done in
Intel hardware.

As previously mentioned, the paging unit translates pages as the smallest unit
and not individual addresses. The default page size on Linux is 4KB (2^12
bytes). So the 20 most significant bits of a 32-bit address alone can uniquely
identify a page (the last 12 bits will be within a page). An rudimentary idea of
implementing paging would be to have a table of (linear page, physical page)
tuples, and try to match the 20 most significant bits of a linear address in the
table. However, that implies that we would need 2^20 entries in the array, or at
least _reserve space_ for that many entries if the implementation is a linear
array, even when the process doesn't use all addresses in the range. With each
entry taking up 4 bytes (32 bits), our table would take 4MB of memory. Of course
there is room for improvement.

The idea is to split the 20 most significant bits into two 10-bit segments; the
first 10 to be used as index to a higher level Page Directory whose entries are
physical addresses to secondary Page Tables, and the rest 10 bits as index to a
particular Page Table. With _demand paging_ (page frames are only assigned when
actually requested), the secondary Page Tables are added as the process requests
more memory. In the extreme case that the process uses all 4GB address space,
the total RAM used for the Page Directory and all Page Tables will be 4MB, no
more than the basic algorithm above.

This is called 2-level paging and you will see the idea getting generalized into
multi-level paging on systems with bigger address spaces, e.g., 64 bit systems.

The physical address of the Page Directory is stored in `cr3`. From there the
paging unit gets the physical address to a Page Table, and in turn it gets the
physical address of a page frame a page is mapped into. Then by adding the least
significant 12 bits of a linear address (`Offset`), it gets a 32 bit physical
address.

## Page Directory/Table Data Structure
In Linux, it is a 32-bit compound integer of the following bits:

_Present flag_. If set, the referred-to page or page table is in main memory. If
not, the linear address requested is saved to `cr2` and exception 14 (Page
Fault) is generated.  The kernel relies on page faults to assign additional page
frames to a process.

_20 most significant bits of a page frame physical address_. Because of the 4KB
page size, the last 12 bits of a physical address doesn't matter.

_Accessed flag_. Whether or not the referred-to page or page table is accessed.
This flag may be used to decide which pages to swap out in case the system is
running low on memory.

_Dirty flag_. Whether or not the referred-to page or page table is written to.
Similar use as the _accessed flag_.

_Read/Write flag_. Access right of the page frame or page table: read-only or
writable.

_User/Supervisor flag_. Privilege needed to access the page or page table: user
process or kernel.

_PCD and PWT flags_. Controls some hardware cache behaviors. Explained below at
"Hardware Cache".

_Page Size flag_. Controls the page size, if not 4KB. See "Extended Paging" for
details.

_Global flag_. Controls TLB flush behaviors.

## Extended Paging
On newer 80x86 processors, larger pages are supported (4MB instead of 4KB) and
thus more bits are needed for the Offset part and fewer for Page Directory and
Page Table. In fact, since the Offset takes 22 bits, the 10 most significant
bits all be used as an index into the Page Directory, thus eliminating the need
for secondary Page Tables.
## Hardware Protection Scheme
Different from the segmentation unit, which allow multiple privilege levels (0,
1, 2, 3), the paging unit only allows two: User/Supervisor. Furthermore, only
two rights are associated with pages: Read/Write.
## Physical Address Extension (PAE) Paging Mechanism
The amount of RAM supported by a processor is limited by the number of _address
pins_ connected to the address bus. As the servers grow, Intel extended the
address pins of their 80x86 processors from 32 to 36. This makes them able to
address up to 2^36 = 64GB memory. Adapting to this change, Linux tweaked the
number of bits used to index Page Directory, Page Table and as Offset, and in
some cases, added an additional table before Page Directory - Page Directory
Pointer Table. But the same general idea applies.

## Paging for 64-bit Architectures
On 64-bi systems, there are 64 address pins. It certainly doesn't make sense to
use all 64-12 = 52 most significant bits (assuming a usual page size of 4KB) as
that would give a maximum of 256TB address space! So fewer bits are used, and
multi-level paging is used. The actual split of bits varies from architecture to
architecture.
## Hardware Cache
Typical RAMs access time is in the order of hundreds of clock cycles. That is a
LOT of time comparing to instruction executions. To remedy the huge speed
mismatch between the CPU and the memory, fast hardware cache memories are
introduced. Based on _locality principle_, data recently fetched from memory, or
those next to them, are kept in the cache. And over the years, multiple levels
of cache are added, each with different speed and cost characteristics.

A new _unit_ was introduced along with the hardware cache: _line_. It refers to
a certain block of contiguous data in memory or in cache. Data are always read
into cache or cleared in multiple of line. A line is usually several dozens of
bytes.

On multi-processor systems, each CPU has its own hardware cache. The
synchronization between two caches is important, and usually is done by a
dedicated hardware unit so the OS doesn't have to care. But one interesting
feature of some cache is that the hardware allows the cache sync policy
(Write-Through v.s. Write-Back) to be selected and set by the OS. In Linux,
Write-Back is always selected.
## Translation Lookaside Buffers (TLB)
Besides general purpose cache, 80x86 processors include another cache called the
_Translation Lookaside Buffer (TLB)_, to speed up linear address translation.
When a linear address is used for the first time, the corresponding physical
address is computed through page table lookups, and stored in the TLB for future
reference. Note that because each process has its own page tables, the TLB is
flushed during a context switch.

On multi-processor systems, each CPU has its own TLB. Unlike hardware cache, two
TLBs don't need to be synchronized because they are being used by two processes
that have different page tables any way.
